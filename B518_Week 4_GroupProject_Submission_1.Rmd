---
title: "B518 | Week 4 | Group Project | Submission 1"
author: "Group 4 - Alex Toon | Nicholas Carlson | Divya Reddy Konda"
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: true
    latex_engine: xelatex
fontsize: 11pt
header-includes:
  - \usepackage{placeins}
params:
---

```{r include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.width = 6.5,
  fig.height = 3.6
)

suppressPackageStartupMessages({
  library(readr)
  library(dplyr)
  library(tidyr)
  library(ggplot2)
  library(knitr)
})

```

# Project Idea One - Covid 19 (2021 ONLY - USA, UK, China, Belgium)

```{r echo=FALSE, dataset_one_import}
dataset_one_URL <- "https://docs.owid.io/projects/covid/en/latest/dataset.html"
dataset_one_URL # to print the URL for the PDF

data1 <- read.csv("data/raw/coviddata.csv")

c(n_rows = nrow(data1), n_cols = ncol(data1))
head(names(data1), 12)

u <- unique(data1$country)
u[grepl("United|USA|UK", u, ignore.case = TRUE)]

#Size
#c(n_rows = nrow(data1), n_cols = ncol(data1))

#Column Names
#head(names(data1))

#Types
#table(sapply(data1, function(x) class(x)[1]))

#quick analysis

#str(data1, list.len = 20)
#summary(data1)
```
# Introduction
This project uses Covid 19 data from 'Our world in data'. We use this to primarily compare how daily new cases per million varied across four countries in 2021. We focus on 2021 to keep our comparisons on a common phase of the pandemic. The dataset itself does cover many more countries and years and also includes data on total cases and total deaths. We used the fields that have the suffix 'per_million'as any comparisons scale by population size. 

# Dataset justification

# Selection Criteria:
The data (see above) does meet the criteria of the assignment. In that it is relevant to health, publically accessible, sizable (61 columns and 530,292 rows), includes both categorical (e.g. country) and continuous variables (e.g. new_cases_per_million, total_deaths_per_million) and finally has been ethically sourced and de-identified. 

**Relevance:** Directly biomedical/public-health, reflecting real-world cases and death metrics during COVID-19.

**Size/structure:** The file far exceeds the minimum requirements (61 columns and 530k rows) and includes both categorical (e.g. Country) and continuous fields (total_deaths_per_million, new_cases_per_million)

**Source Location:** https://docs.owid.io/projects/covid/en/latest/dataset.html
**Raw data Location:** https://catalog.ourworldindata.org/garden/covid/latest/compact/compact.csv

**Accessibility/ethics:** Publicly accessible aggregated, de-identified counts suitable for academic use.

**Analytical potential:** Feasible for tables, histograms, boxplots, time trends. Using the fields with the suffix "per_million" allows better scaling for cross country comparisons and summaries.

**Ethical use.** The dataset consists of aggregated, de-identified counts without PII; no patient-level identifiers are present, aligning with course requirements for ethical, public data.
  
# Variables and structure
This analysis focuses on a few key variables from the dataset. The primary categorical variable is 'country', which we have filtered to four specific nations. The main continuous variable is 'new_cases_per_million', which allows for a fair comparison of infection rates by account for population differences. Finally, the 'date' variable was used to filter the data to the 2021 calendar year. 

A list of all the fields:
- "country" 
- "date" 
- "total_cases", "total_cases_per_million"
- "new_cases", "new_cases_smoothed", "new_cases_per_million", "new_cases_smoothed_per_million"
- "total_deaths", "new_deaths", "new_deaths_smoothed", "total_deaths_per_million" 
  
# Research questions
- 1. What share of days exceed a threshold (to simulated a government policy threshold to "flatten the curve") e.g 50 cases per million in each country
- 2. Which of the selected countries had the highest typical daily new cases per million in 2021
- 3. How did the monthly mean of new cases per million over 2021 for each country

# Data clean up & Processing plan
We parsed the date field and derived a 'year' variable, then restricted the dataset to 2021 to keep figures more legible and comparable. We fixed our analysis to a small set of countries (United States, United Kingdom, China, Belgium) and then verified each has sufficient non missing values for 'new_cases_per_million' in 2021. this processing prepares the data for descriptive statistics and many visualisations.    
```{r echo=FALSE, results='hide', data_clean_up}

#check year is available & parse
data1$date <- as.Date(data1$date)
data1$year <- as.integer(format(data1$date, "%Y"))

#filter by one year to reduce dataset. Focusing on year 2021
d1_21 <- subset(data1, year ==2021)


# lock in selected countries
keep_countries <- c("United States", "United Kingdom", "China", "Belgium")

# Set Threshold
threshold <- 50


# Only keep countries with enough non missing values for 'new_cases_per_million'
ok_counts <- tapply(!is.na(d1_21$new_cases_per_million), d1_21$country, sum)

#show counts for chosen countries
counts_chosen <- ok_counts[keep_countries]
counts_chosen

d1_21_f <- subset(d1_21,
            country %in% keep_countries,
            select = c(country, date, new_cases_per_million))

d1_21_f$country <- factor (d1_21_f$country, levels = keep_countries)

```

# Descriptive statistics (figures in Appendix)
 Our descriptive analysis for 2021 reveals starkly different pandemic experiences among the four selected countries. The most significant finding is the extreme contrast between China, which reported virtually no community spread, and the western nations all expereicned substantial waves of the Covid virus. 
 
 Answering our first research question, the two way frequency shows that China had reported zero days exceeding a 50 new cases per million threshold. In contrast this threshold was crossed 94.% of days in Belgium, 89.9% in the USA and 84.9% in the UK. 
 
 For our second question, the summary statistics table identified the UK as having the highesy typical daily caseload, with a median of 421.6 new cases per million, nearly double that of the USA at 218.4. The overall distrubution o cases is heavily right skewed, a pattern confirmed visually by the histogram and the numerous high end outliers visibl in the boxplot. 
 
 Lastly, for our third research question. the time trend plot issustrates how these case rates evolved monthly. China's rate remained flat, the US, UK and Belgium all expereicned a summer drop followed by a big surge in Q4 of 2021. 
 
## Boxplot (numeric by category)
Boxplot (Mortality rate by category) 
```{r echo=FALSE, boxplot}
# Boxplot

  # Order by median
  d1_21_f$country <- reorder(d1_21_f$country, d1_21_f$new_cases_per_million, median, na.rm = TRUE)

  # % of high days per country
  ## Reusing previously set threshold of '50'
  
  d_tmp <- subset(d1_21_f, !is.na(new_cases_per_million))
  pct_high <- tapply(d_tmp$new_cases_per_million > threshold,
                     d_tmp$country, function(z) mean(z, na.rm = TRUE))
  pct_high <- round(100 * pct_high[levels(d1_21_f$country)], 0)

  
  lab <- paste0(levels(d1_21_f$country), "\n", pct_high, "% >", threshold)
  
  #Box plot
  bp <- boxplot(new_cases_per_million ~ country, data = d1_21_f,
          names = lab,
          main = "Daily new cases per million, by country in 2021",
          ylab = "New cases per Million",
          xlab = "Country",
          col = "lightgreen",
          las = 2, 
          outline = TRUE,
          border = "purple")
  
  #Percentiles
  q <- quantile(d1_21_f$new_cases_per_million, probs = c(0.25, 0.5, 0.75), na.rm = TRUE)

  # Adding a horizontal line for the quarters
  abline(h = q, col = c("blue", "red", "orange"), lty = c(2, 3, 2))
  
```
## Time trend (average by year)

```{r echo=FALSE, timetrend}

#month index
d1_21_f$month_num <- as.integer(format(d1_21_f$date, "%m"))

# Monthly means by country
m_means <- aggregate(new_cases_per_million ~ country + month_num,
                     data = d1_21_f, FUN = function(z) mean(z, na.rm = TRUE))


countries = c("United States", "United Kingdom", "China","Belgium")
mat <- with (m_means, tapply(new_cases_per_million, list(month_num, country), mean))
mat <- mat[1:12, countries, drop = FALSE]

# Set highest point to scale chart
y_max <- max(mat, na.rm = TRUE)

plot(1:12, mat[, "United States"], type = "l", lwd = 2,
     xaxt = "n", xlab = "Month in 2021",
     ylab = "mean new cases per million",
     main = "Monthly mean new cases per million",
     ylim = c(0, y_max * 1.1))

axis(1, at = 1:12, labels = month.abb)
lines(1:12, mat[, "United Kingdom"], lwd = 2)
lines(1:12, mat[, "China"], lwd = 2)
lines(1:12, mat[, "Belgium"], lwd = 2)

legend("topleft",
       legend = countries, 
       lwd = 2, bty = "n")

```

# Planned statisical methods
To formally test for differences in the median daily new cases between the four countries, we plan to use a non-parametric test such as the Kruskal-Wallis test, given the skewed nature of the data. Further analysis could involve using correlation to explore the relationship between vaccination rates and new cases over time for each country. 

# Limitations
- Measurement differences - countries have different reporting rules, testing cadence & breadth.
- Scope - Only 2021 was analysed. Other years or waves of the disease may show other patterns. 
- per million rates do not adjust for demographics of each country, which may show other patterns. 
- China has several near zero analysis - This may reflect reporting practices of this specific country

# Appendix - Project One
## One-Way frequency table (categorical)
Counts and proportions for a categorical variable 
```{r echo=FALSE, onewaytable}
# Counts and proportions for country
tab_country <- table(d1_21_f$country)
prop_country <- prop.table(tab_country)

tab_country
round(prop_country, 3)
```
## Bar Chart of Disease.Category (counts)
Bar chart / Bar plot of disease category by count 
```{r echo=FALSE, barplot}
# Bar chart / Bar plot of disease category by count
med_cases <- tapply(d1_21_f$new_cases_per_million,
                    d1_21_f$country,
                    median, na.rm = TRUE)

barplot(med_cases,
        main = "Median daily new cases per million (2021)", 
        xlab = "Country",
        ylab = "Median new cases per million",
        las = 2)

```
## Two way table (category by category)
  
```{r echo=FALSE, twowaytable}
# Cross-tab and row/column proportions

d_tw <- subset(d1_21_f, !is.na(new_cases_per_million))

d_tw$high_day <- d_tw$new_cases_per_million > threshold

tw <- table(d_tw$country, d_tw$high_day)
tw

round(prop.table(tw, margin = 1), 3)
round(prop.table(tw, margin = 2), 3)

addmargins(tw)
row_high <- round(prop.table(tw, margin = 1)[, "TRUE"], 3)
row_high

```
## Center & Spread (overall, selected countries, 2021)

```{r echo=FALSE, center_and_spread}



x <- d1_21_f$new_cases_per_million
x <- x[!is.na(x)]

overall_stats <- c(
  mean = mean(x),
  median = median(x),
  min = min(x),
  max = max(x),
  range = diff(range(x)),
  IQR = IQR(x),
  sd = sd(x))

round(overall_stats[c("median","IQR","sd")], 1)


#By Countries summaries 
by_med <- tapply(d1_21_f$new_cases_per_million, d1_21_f$country, median, na.rm = TRUE)
by_iqr <- tapply(d1_21_f$new_cases_per_million, d1_21_f$country, IQR, na.rm = TRUE)
by_sd <- tapply(d1_21_f$new_cases_per_million, d1_21_f$country, sd, na.rm = TRUE)


by_summ <- data.frame(
    country = names(by_med),
    median = round(by_med, 1),
    IQR = round(by_iqr, 1),
    sd = round(by_sd, 1),
    row.names = NULL
)

print(by_summ)

```
## Histogram (shape of the distrubution)
  
```{r echo=FALSE, histogram}

#Dropping NA's
x <- d1_21_f$new_cases_per_million
x <- x[!is.na(x)]

# Looking at the shape of the distrubution through a histogram
h <- hist(x ,
          probability = TRUE, # Relative frequency histogram
          main = "Daily new cases per million",
          xlab = "New cases per million",
          ylab = "Relative Frequency",
          breaks = 50,
          xlim = c(0,1500),
          col = "purple",
          border = "black")

# Add frequency polygon (connects midpoints of histogram bins)
  lines(h$mids, h$density, type = "l", col = "red", lwd = 2)

# Adding a smooth density curve for comparison
  lines(density(x), col = "orange", lwd = 2, lty = 2)

# Add legend
legend("topright",
       legend = c("Density", "Density Curve"),
       col = c("red", "orange"),
       lwd = 2,
       lty = c(1, 2))

```

\FloatBarrier
\newpage 

# Project Idea Two - Covid 19 Hospitalizations in France
# Link to the dataset

Kaggle - Coronavirusdataset France (file: `chiffres-cles.csv`)  
Actual URL: <https://www.kaggle.com/datasets/mclikmb4/coronavirusdataset-france?select=chiffres-cles.csv>
Google drive URL: <https://drive.google.com/file/d/1rXHdGEDWFAMaitmkNSgehAt_e2FaC_PZ/view?usp=sharing>

# Introduction to the dataset

This dataset provides daily COVID-19 surveillance indicators for France at multiple geographic granularities (country, region, department, overseas collectivities). Each record includes a calendar date, a location code, and a location name, enabling comparisons across space and time. Indicators cover hospitalized patients, ICU occupancy, cumulative deaths, cumulative recoveries, and daily flows of new admissions (hospital and ICU). Source/provenance fields support auditability. The structure suits descriptive analyses and visualizations, with optional regional comparisons to highlight spatial heterogeneity. These indicators and their definitions are documented on the Kaggle dataset page (mclikmb4, 2020-2021).

# Dataset justification

**Relevance:** Directly biomedical/public-health, reflecting real-world hospital and ICU loads during COVID-19.

**Size/structure:** The file far exceeds the minimum requirements (well over 100 rows and more than 20 columns) and includes both categorical (granularity, location IDs, sources) and continuous (counts) variables.

**Accessibility/ethics:** Publicly accessible aggregated, de-identified counts suitable for academic use.

**Analytical potential:** Enables trend estimation, wave identification, geographic comparison, and lead-lag analysis between admissions (“flow”) and occupancy (“stock”).


**Ethical use.** The dataset consists of aggregated, de-identified counts without PII; no patient-level identifiers are present, aligning with course requirements for ethical, public data.


# Variables description

**Key columns:**  
`date` (daily), `granularity` (country, region, department), `location_code` (location code), `location_name` (location name).

**Indicators:**  
- `hospitalized` - current hospitalized patients  
- `icu_patients` - current ICU patients  
- `deaths` - cumulative deaths  
- `recovered` - cumulative recoveries  
- `new_hospitalizations` - new daily hospital admissions  
- `new_icu_admissions` - new daily ICU admissions  

**Additional fields:**  
`confirmed_cases` and `tested` may be present with different levels of completeness.  
**Note:** Due to several missing/invalid values (NaN/Inf), the `tested` column is largely unusable for analysis and is excluded from primary summaries and plots.

**Source metadata:**  
`source_name`, `source_url`, `source_archive`, `source_type`.

```{r results='hide'}
df <- readr::read_csv("https://drive.google.com/uc?export=download&id=1rXHdGEDWFAMaitmkNSgehAt_e2FaC_PZ", show_col_types = FALSE)

df <- df |>
  dplyr::mutate(
    granularity = dplyr::recode(granularity,
      "pays" = "country",
      "departement" = "department",
      "collectivite-outremer" = "overseas_collectivity",
      "monde" = "world",
      "region" = "region"
    )
  )


# Coerce likely numeric metrics
num_cols <- intersect(
  c("hospitalized","icu_patients","deaths","recovered",
    "new_hospitalizations","new_icu_admissions",
    "confirmed_cases","tested"),
  names(df)
)
df <- df |>
  mutate(
    date = as.Date(date),
    across(all_of(num_cols), ~suppressWarnings(as.numeric(.)))
  )

# Overview table for structure (TABLE ONLY)
gran_counts <- df |>
  count(granularity, name = "n") |>
  arrange(desc(n))

# Flattened numeric summary table (no list-columns)
summary_table <- df |>
  summarize(across(
    all_of(num_cols),
    list(
      n = ~sum(!is.na(.x)),
      mean = ~mean(.x, na.rm = TRUE),
      sd = ~sd(.x, na.rm = TRUE),
      median = ~median(.x, na.rm = TRUE),
      min = ~min(.x, na.rm = TRUE),
      max = ~max(.x, na.rm = TRUE)
    ),
    .names = "{.col}_{.fn}"
  )) |>
  pivot_longer(
    everything(),
    names_to = c("variable","stat"),
    names_sep = "_(?=[^_]+$)",
    values_to = "value"
  ) |>
  pivot_wider(names_from = "stat", values_from = "value") |>
  arrange(variable)
```

```{r}
kable(gran_counts, caption = "Row counts by geographic granularity")
kable(summary_table, digits = 3, caption = "Summary statistics for key numeric indicators")
```

# Research question(s)

1. **National waves:** How did France’s national hospitalization and ICU occupancy evolve across early pandemic waves (2020-2021)?  
2. **Flow-stock timing:** Do peaks in new hospital admissions precede peaks in current hospitalizations, and by roughly how many days?

# Data cleanup and processing plan

- **Parsing and types:** Ensure the `date` field is properly parsed as a date variable and convert indicator fields into numeric types for consistency.

- **Subsetting:** For national trends, include only rows classified as country with `location_code` = "FRA". For geographic comparisons, restrict the dataset to rows where `granularity` is region.

- **Missingness:** Quantify missing values for each column and handle them transparently by applying listwise deletion for plotted series (no imputation).

- **Duplicates:** Identify and remove duplicate entries defined by the combination of `date` and `location_code`.

- **Provenance:** Retain all source metadata fields, and include them in the appendix when relevant for transparency.

# Descriptive statistics (figures in Appendix)

France’s national indicators exhibit multi-wave patterns during 2020-2021. Hospital occupancy and ICU burden rise and fall in tandem with case surges, while cumulative deaths increase monotonically. The timing relationship between new admissions (flow) and current occupancy (stock) suggests admissions lead occupancy by several days. For visuals supporting these statements, see Appendix Figures A1-A3. Tables above summarize structure and central tendencies.

Across all rows, the median current hospitalizations was `r round(median(df$hospitalized, na.rm = TRUE))`, with an IQR of `r paste(round(quantile(df$hospitalized, probs = c(.25,.75), na.rm = TRUE)), collapse = "-")`; ICU occupancy had a much lower median, which is expected since ICU is a subset of the total hospital (median `r round(median(df$icu_patients, na.rm = TRUE))`), consistent with ICU being a subset of total hospital burden.

# Planned statistical methods

- **Lagged cross-correlation** between `new_hospitalizations` (flow) and `hospitalized` (stock) to estimate lead time from admissions to occupancy.  
- **Regional comparison** of ICU vs hospital burden by wave period (medians, IQRs).  
- **Simple time-series decomposition** on national hospitalizations to separate trend/seasonal/residual components (if applicable).


# Limitations

Several fields like `tested` and early `confirmed_cases` have bad coverage over time, and indicators are hospital-centric rather than community-representative. Counts are aggregated and de-identified, so patient-level cannot be controlled. Because the dataset mixes granularities (national, regional, departmental), comparing across levels requires careful subsetting (`granularity == "country"` for national trends). These constraints limit causal interpretation, so we have to focus more on descriptive trends and clearly labeled comparisons.

# Appendix - Project Two

```{r results='hide'}
national <- df |>
  filter(granularity == "country", location_code == "FRA") |>
  arrange(date)

plot_series <- function(data, y, title_txt, ylab_txt = "Count") {
  ggplot(data, aes(x = date, y = .data[[y]])) +
    geom_line(color = "blue", linewidth = 0.8) +
    scale_x_date(date_breaks = "2 months",
                 date_labels = "%Y-%m",
                 expand = expansion(mult = c(0.01, 0.01))) +
    labs(x = "Date", y = ylab_txt, title = title_txt) +
    theme_minimal(base_size = 10) +
    theme(
      plot.title = element_text(hjust = 0.5),
      axis.title.y = element_text(margin = margin(r = 6))
    )
}

```

```{r echo=FALSE, fig.cap="France (national): Hospitalized (current) over time."}
if (all(c("hospitalized") %in% names(national))) {
  plot_series(national, "hospitalized", "France (national): Hospitalized (current) over time", ylab_txt = "Patients")
}
```

```{r echo=FALSE, fig.cap="France (national): ICU (current) over time."}
if (all(c("icu_patients") %in% names(national))) {
  plot_series(national, "icu_patients", "France (national): ICU (current) over time", ylab_txt = "Patients in ICU")
}
```

```{r echo=FALSE, fig.cap="France (national): Cumulative deaths over time."}
if (all(c("deaths") %in% names(national))) {
  plot_series(national, "deaths", "France (national): Cumulative deaths over time", ylab_txt = "Deaths (cumulative)")
}
```

\FloatBarrier
\newpage


# Project Idea Three - Heart attack


# Link to the dataset

https://www.kaggle.com/datasets/iamsouravbanerjee/heart-attack-prediction-dataset

# Introduction to dataset

The Heart Attack Prediction Dataset, available on Kaggle, is a comprehensive resource for studying the clinical, lifestyle, and demographic factors associated with cardiovascular risk. It consists of 8,763 de-identified patient records, including continuous variables such as age, cholesterol, blood pressure, and heart rate, as well as categorical features like sex, chest pain type, smoking habits, diabetes status, and dietary patterns. Socioeconomic and geographic attributes, including income and region, further enrich the dataset by adding broader context to heart health predictors. The primary outcome variable indicates whether a patient is at risk of a heart attack, making the dataset well-suited for statistical analysis, visualization, and classification tasks. Its diverse mix of variables supports exploration of correlations, risk factors, and group comparisons, while also providing an ethical and accessible foundation for predictive modeling in cardiovascular health research.

# Dataset justification

I chose the Heart Attack Prediction Dataset because it directly addresses a critical biomedical challenge cardiovascular disease which remains one of the leading causes of mortality worldwide. The dataset integrates clinical, lifestyle, and demographic variables, making it highly relevant for exploring the multifactorial nature of heart health. With its balanced mix of categorical and continuous features, it offers strong potential for applying a variety of statistical methods, visualizations, and predictive modeling techniques. Its size and diversity of attributes make it complex enough to yield meaningful insights, yet still manageable for academic analysis. Overall, this dataset provides both real-world relevance and analytical richness, making it an excellent candidate for this project.

# Variables description

Key columns include Patient ID (unique identifier for each record), Age (in years), Sex (male or female), Cholesterol (cholesterol levels in mg/dL), Blood Pressure (systolic/diastolic in mmHg), Heart Rate (beats per minute), and BMI (body mass index, kg/m²). Clinical indicators capture Diabetes status (Yes/No), Family History of heart problems (1 = Yes, 0 = No), Previous Heart Problems (1 = Yes, 0 = No), Medication Use (1 = Yes, 0 = No), and Triglyceride levels (mg/dL). Lifestyle-related attributes include Smoking (1 = Smoker, 0 = Non-smoker), Obesity (1 = Obese, 0 = Not obese), Alcohol Consumption (None, Light, Moderate, Heavy), Diet (Healthy, Average, Unhealthy), Exercise Hours Per Week, Physical Activity Days Per Week, Stress Level (1–10 scale), Sedentary Hours Per Day, and Sleep Hours Per Day. Socioeconomic and demographic fields consist of Income, Country, Continent, and Hemisphere. The target variable, Heart Attack Risk, is a binary indicator (1 = Yes, 0 = No) denoting whether the patient is at risk of a heart attack.

```{r}
# Load ggplot2 for visualizations
library(ggplot2)

# Load dataset
hd <- read.csv("heart_attack_prediction_dataset.csv")

# View dataset structure and summary
str(hd)
summary(hd)

# Boxplot of Age grouped by Heart Attack Risk
ggplot(hd, aes(x = factor(Heart.Attack.Risk), y = Age, fill = factor(Heart.Attack.Risk))) +
  geom_boxplot() +
  labs(title = "Age Distribution by Heart Attack Risk",
       x = "Heart Attack Risk (0 = No, 1 = Yes)", y = "Age (years)") +
  scale_fill_manual(values = c("lightgreen", "red")) +
  theme_minimal()

# Bar plot of a categorical variable (Smoking)
ggplot(hd, aes(x = factor(Smoking))) +
  geom_bar(fill = "orange", color = "black") +
  labs(title = "Smoking Status of Patients",
       x = "Smoking (0 = Non-Smoker, 1 = Smoker)", y = "Count")

```

# Research questions
1. Which clinical, lifestyle, and demographic factors are most strongly associated with the risk of heart attack in patients?


2. Which features contribute most to a machine learning model’s decision boundary for predicting heart attack risk?

# Data cleanup and processing plan

- Check for missing values: Identify NAs using colSums(is.na(hd)); if very few, remove those rows; if moderate, impute using mean/median for continuous variables (e.g., Cholesterol, BMI) and mode for categorical variables (e.g., Diet, Alcohol Consumption).

- Remove duplicate entries: Drop exact duplicates or repeated Patient IDs to avoid over-representation using hd <- hd[!duplicated(hd), ].

- Fix inconsistent formats: Split Blood Pressure into two numeric columns (Systolic and Diastolic) and convert binary indicators (0/1) like Diabetes, Smoking, and Heart Attack Risk into categorical factors.

- Validate ranges & handle outliers: Review continuous variables (e.g., Cholesterol, Triglycerides, BMI, Sleep Hours) for biologically implausible values; correct, cap, or remove extreme outliers as appropriate.

- Standardize categorical variables: Ensure consistent levels for Sex (Male/Female), Diet (Healthy/Average/Unhealthy), and Alcohol Consumption (None/Light/Moderate/Heavy).

- Create derived variables: Add new groupings such as Age Groups (e.g., 18–30, 31–50, 51–70, 71–90) and BMI Categories (Underweight, Normal, Overweight, Obese) to facilitate group comparisons in descriptive statistics and visualization.

# Descriptive statistics and data visualizations

```{r}

# Load dataset
hd <- read.csv("heart_attack_prediction_dataset.csv")

# --- Continuous variables: Mean, Median, Range, SD ---
continuous_vars <- c("Age", "Cholesterol", "Heart.Rate", "BMI", "Triglycerides","Exercise.Hours.Per.Week", "Stress.Level","Sedentary.Hours.Per.Day", "Income", 
"Physical.Activity.Days.Per.Week", "Sleep.Hours.Per.Day")

for (var in continuous_vars) {
  cat("\nVariable:", var, "\n")
  cat("Mean:", mean(hd[[var]], na.rm=TRUE), "\n")
  cat("Median:", median(hd[[var]], na.rm=TRUE), "\n")
  cat("Range:", diff(range(hd[[var]], na.rm=TRUE)), "\n")
  cat("Standard Deviation:", sd(hd[[var]], na.rm=TRUE), "\n")
}

# --- Categorical variables: Frequency counts & proportions ---
categorical_vars <- c("Sex", "Diabetes", "Family.History", "Smoking", "Obesity","Alcohol.Consumption", "Diet", "Previous.Heart.Problems","Medication.Use", "Country", "Continent", "Hemisphere","Heart.Attack.Risk")

for (var in categorical_vars) {
  cat("\nVariable:", var, "\n")
  print(table(hd[[var]]))             
}

```

# Distribution of Heart Attack Risk Across Demographic and Lifestyle Factors

```{r}
# Load library
library(ggplot2)

# Load dataset
hd <- read.csv("heart_attack_prediction_dataset.csv")

# BMI distribution by Heart Attack Risk 
ggplot(hd, aes(x = BMI, fill = factor(Heart.Attack.Risk))) +
  geom_histogram(position = "dodge", bins = 30, alpha = 0.7) +
  labs(title = "BMI Distribution by Heart Attack Risk",
       x = "BMI (kg/m²)", 
       y = "Count", 
       fill = "Heart Attack Risk") +
  theme_minimal()

# Age distribution by Heart Attack Risk 
ggplot(hd, aes(x = Age, fill = factor(Heart.Attack.Risk))) +
  geom_histogram(position = "dodge", bins = 30, alpha = 0.7) +
  labs(title = "Age Distribution by Heart Attack Risk",
       x = "Age (years)", 
       y = "Count", 
       fill = "Heart Attack Risk") +
  theme_minimal()

# Heart attack risk by gender
ggplot(hd, aes(x = Sex, fill = factor(Heart.Attack.Risk))) +
  geom_bar(position = "fill") +
  geom_text(stat = "count", aes(label = ..count..),
            position = position_fill(vjust = 0.5), color = "black") +
  labs(title = "Heart Attack Risk by Gender",
       x = "Gender",
       y = "Proportion",
       fill = "Heart Attack Risk (0=No, 1=Yes)") +
  theme_minimal()

# Heart Attack Risk by Smoking Status 
ggplot(hd, aes(x = factor(Smoking), fill = factor(Heart.Attack.Risk))) +
  geom_bar(position = "fill") +
  geom_text(stat = "count", aes(label = ..count..),
            position = position_fill(vjust = 0.5), color = "black") +
  labs(title = "Heart Attack Risk by Smoking Status",
       x = "Smoking (0=No, 1=Yes)",
       y = "Proportion",
       fill = "Heart Attack Risk") +
  theme_minimal()

# Heart Attack Risk by Obesity Status
ggplot(hd, aes(x = factor(Obesity), fill = factor(Heart.Attack.Risk))) +
  geom_bar(position = "fill") +
  geom_text(stat = "count", aes(label = ..count..),
            position = position_fill(vjust = 0.5), color = "black") +
  labs(title = "Heart Attack Risk by Obesity Status",
       x = "Obesity (0=No, 1=Yes)",
       y = "Proportion",
       fill = "Heart Attack Risk") +
  theme_minimal()

```

```{r}
library(ggplot2)
library(reshape2) 
library(corrplot)   

numeric_vars <- hd[sapply(hd, is.numeric)]

# Compute correlation matrix
cor_matrix <- cor(numeric_vars, use = "complete.obs")
melted_cor <- melt(cor_matrix)

ggplot(melted_cor, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
  labs(title = "Correlation Matrix Heatmap",
       x = "",
       y = "",
       fill = "Correlation")

```

# Planned statistical methods

As the project progresses, I plan to use chi-square tests to assess associations between categorical factors (e.g., smoking, diabetes) and heart attack risk, and t-tests/ANOVA to compare continuous measures (e.g., cholesterol, BMI) across groups. To build predictive insight, I will apply logistic regression and may explore machine learning models such as decision trees or random forests. These methods will help identify key risk factors and evaluate their predictive power.

# Limitations 

While descriptive statistics provide valuable insights into the dataset, they also have some limitations. Measures like mean and standard deviation can be influenced by outliers, which may distort the true central tendency and variability of the data. Categorical variables summarized with frequency counts may oversimplify complex health behaviors, such as smoking or alcohol consumption, without capturing intensity or duration. The dataset itself may contain missing values, inconsistencies, or biases due to self-reported measures (e.g., diet, stress level, or exercise). Additionally, descriptive statistics do not establish causal relationships; they only describe patterns. Therefore, more advanced statistical methods and inferential analyses are needed to draw meaningful conclusions about risk factors for heart attacks.

# Appendix - Project Three


# ) JOINT PROJECTS - References
- Project 1 - Our World in Data. (2024). Coronvirus Pandemic (COVID-19) dataset. <https://docs.owid.io/projects/covid/en/latest/dataset.html>
- Project 2 - mclikmb4, (2021, April 4), Coronavirus-dataset France, Kaggle, <https://www.kaggle.com/datasets/mclikmb4/coronavirusdataset-france?select=chiffres-cles.csv> 
- Project 3 - Banerjee, S. (2021). Heart Attack Prediction Dataset. Kaggle. https://www.kaggle.com/datasets/iamsouravbanerjee/heart-attack-prediction-dataset