---
title: "B518 | Week 4 | Group Project | Submission 1"
author: "Group 4 - Alex Toon | Nicholas Carlson | Divya Reddy Konda"
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: true
    latex_engine: xelatex
fontsize: 11pt
header-includes:
  - \usepackage{placeins}
params:
---

```{r include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.width = 6.5,
  fig.height = 3.6
)

suppressPackageStartupMessages({
  library(readr)
  library(dplyr)
  library(tidyr)
  library(ggplot2)
  library(knitr)
})

```

# Project Idea One - Covid 19 (2021 ONLY - USA, UK, China, Belgium)
## 1) Original Source: URL: https://docs.owid.io/projects/covid/en/latest/dataset.html
## 2) Dataset moved:  https://catalog.ourworldindata.org/garden/covid/latest/compact/compact.csv

### Selection Criteria:
#### The data (see above) does meet the criteria of the assignment. In that it is relevant to health, publically accessible, sizable (61 columns and 530,292 rows), includes both categorical (e.g. country) and continuous variables (e.g. new_cases_per_million, total_deaths_per_million) and finally has been ethically sourced and de-identified. 



```{r echo=FALSE, dataset_one_import}
dataset_one_URL <- "https://docs.owid.io/projects/covid/en/latest/dataset.html"
dataset_one_URL # to print the URL for the PDF

data1 <- readr::read_csv("https://catalog.ourworldindata.org/garden/covid/latest/compact/compact.csv" , show_col_types = FALSE, progress = FALSE)

c(n_rows = nrow(data1), n_cols = ncol(data1))
head(names(data1), 12)

u <- unique(data1$country)
u[grepl("United|USA|UK", u, ignore.case = TRUE)]
```


## 2) Introduction (4-6 Sentences)
This project uses Covid 19 data from 'Our world in data'. We use this to primarily compare how daily new cases per million varied across four countries in 2021. We focus on 2021 to keep our comparisons on a common phase of the pandemic. The dataset itself does cover many more countries and years and also includes data on total cases and total deaths. We used the fields that have the suffix 'per_million'as any comparisons scale by population size. 

## 3) Why this dataset? (1 paragraph)
 This dataset is highly relevant to health outcomes. The dataset is also very well documented, large (61 columns and 530k rows). For analysis potential, this dataset has both continuous fields (total_deaths_per_million, new_cases_per_million)  and categorical (e.g. Country), feasible for tables, histograms, boxplots, time trends. Using the fields with the suffix "per_million" allows better scaling for cross country comparisons and summaries. 
  
## 4) Variables and structure
  
```{r echo=FALSE, results='hide', Variables_and_structure}

#Size
c(n_rows = nrow(data1), n_cols = ncol(data1))

#Column Names
head(names(data1))

#Types
table(sapply(data1, function(x) class(x)[1]))

#quick analysis

str(data1, list.len = 20)
head(data1, 10)
summary(data1)

```
## 5) Research questions
- 1. What share of days exceed a threshold (to simulated a government policy threshold to "flatten the curve") e.g 50 cases per million in each country
- 2. Which of the selected countries had the highest typical daily new cases per million in 2021
- 3. How did the monthly mean of new cases per million over 2021 for each country

## 6) Data clean up & Processing plan
We parsed the date field and derived a 'year' variable, then restricted the dataset to 2021 to keep figures more legible and comparable. We fixed our analysis to a small set of countries (United States, United Kingdom, China, Belgium) and then verified each has sufficient non missing values for 'new_cases_per_million' in 2021. this processing prepares the data for descriptive statistics and many visualisations.    
```{r echo=FALSE, data_clean_up}

#check year is available & parse
data1$date <- as.Date(data1$date)
data1$year <- as.integer(format(data1$date, "%Y"))

#filter by one year to reduce dataset. Focusing on year 2021
d1_21 <- subset(data1, year ==2021)


# lock in selected countries
keep_countries <- c("United States", "United Kingdom", "China", "Belgium")


# Only keep countries with enough non missing values for 'new_cases_per_million'
ok_counts <- tapply(!is.na(d1_21$new_cases_per_million), d1_21$country, sum)

#show counts for chosen countries
counts_chosen <- ok_counts[keep_countries]
counts_chosen

d1_21_f <- subset(d1_21,
            country %in% keep_countries,
            select = c(country, date, new_cases_per_million))

d1_21_f$country <- factor (d1_21_f$country, levels = keep_countries)

```

## 7) Descriptive statistics & visualisations
 We summarise categories (counts/proportions), report center & spread for one mumeric variable and add simple plots to visualise patterns 
### 7.1) One-Way frequency table (categorical)
 Counts and proportions for a categorical variable 
```{r echo=FALSE, onewaytable}
# Counts and proportions for country
tab_country <- table(d1_21_f$country)
prop_country <- prop.table(tab_country)

tab_country
round(prop_country, 3)
```
  


### 7.2) Bar Chart of Disease.Category (counts)
 Bar chart / Bar plot of disease category by count 
```{r echo=FALSE, barplot}
# Bar chart / Bar plot of disease category by count
med_cases <- tapply(d1_21_f$new_cases_per_million,
                    d1_21_f$country,
                    median, na.rm = TRUE)

barplot(med_cases,
        main = "Median daily new cases per million (2021)", 
        xlab = "Country",
        ylab = "Median new cases per million",
        las = 2)

```
  


### 7.3) Two way table (category by category)
  
```{r echo=FALSE, twowaytable}
# Cross-tab and row/column proportions
threshold <- 50

d_tw <- subset(d1_21_f, !is.na(new_cases_per_million))

d_tw$high_day <- d_tw$new_cases_per_million > threshold

tw <- table(d_tw$country, d_tw$high_day)
tw

round(prop.table(tw, margin = 1), 3)
round(prop.table(tw, margin = 2), 3)

addmargins(tw)
row_high <- round(prop.table(tw, margin = 1)[, "TRUE"], 3)
row_high

```
  


### 7.4) Center & Spread (overall, selected countries, 2021)
  
```{r echo=FALSE, center_and_spread}



x <- d1_21_f$new_cases_per_million
x <- x[!is.na(x)]

overall_stats <- c(
  mean = mean(x),
  median = median(x),
  min = min(x),
  max = max(x),
  range = diff(range(x)),
  IQR = IQR(x),
  sd = sd(x))

round(overall_stats[c("median","IQR","sd")], 1)


#By Countries summaries 
by_med <- tapply(d1_21_f$new_cases_per_million, d1_21_f$country, median, na.rm = TRUE)
by_iqr <- tapply(d1_21_f$new_cases_per_million, d1_21_f$country, IQR, na.rm = TRUE)
by_sd <- tapply(d1_21_f$new_cases_per_million, d1_21_f$country, sd, na.rm = TRUE)


by_summ <- data.frame(
    country = names(by_med),
    median = round(by_med, 1),
    IQR = round(by_iqr, 1),
    sd = round(by_sd, 1),
    row.names = NULL
)

print(by_summ)

```
  


### 7.5) Histogram (shape of the distrubution)
  
```{r echo=FALSE, histogram}

#Dropping NA's
x <- d1_21_f$new_cases_per_million
x <- x[!is.na(x)]

# Looking at the shape of the distrubution through a histogram
h <- hist(x ,
          probability = TRUE, # Relative frequency histogram
          main = "Daily new cases per million",
          xlab = "New cases per million",
          ylab = "Relative Frequency",
          breaks = 50,
          xlim = c(0,1500),
          col = "purple",
          border = "black")

# Add frequency polygon (connects midpoints of histogram bins)
  lines(h$mids, h$density, type = "l", col = "red", lwd = 2)

# Adding a smooth density curve for comparison
  lines(density(x), col = "orange", lwd = 2, lty = 2)

# Add legend
legend("topright",
       legend = c("Density", "Density Curve"),
       col = c("red", "orange"),
       lwd = 2,
       lty = c(1, 2))

```
  


### 7.6) Boxplot (numeric by category)
 Boxplot (Mortality rate by category) 
```{r echo=FALSE, boxplot}
# Boxplot

  # Order by median
  d1_21_f$country <- reorder(d1_21_f$country, d1_21_f$new_cases_per_million, median, na.rm = TRUE)

  # % of high days per country
  ## Reusing previously set threshold of '50'
  
  d_tmp <- subset(d1_21_f, !is.na(new_cases_per_million))
  pct_high <- tapply(d_tmp$new_cases_per_million > threshold,
                     d_tmp$country, function(z) mean(z, na.rm = TRUE))
  pct_high <- round(100 * pct_high[levels(d1_21_f$country)], 0)

  
  lab <- paste0(levels(d1_21_f$country), "\n", pct_high, "% >", threshold)
  
  #Box plot
  bp <- boxplot(new_cases_per_million ~ country, data = d1_21_f,
          names = lab,
          main = "Daily new cases per million, by country in 2021",
          ylab = "New cases per Million",
          xlab = "Country",
          col = "lightgreen",
          las = 2, 
          outline = TRUE,
          border = "purple")
  
  #Percentiles
  q <- quantile(d1_21_f$new_cases_per_million, probs = c(0.25, 0.5, 0.75), na.rm = TRUE)

  # Adding a horizontal line for the quarters
  abline(h = q, col = c("blue", "red", "orange"), lty = c(2, 3, 2))
  
```
  


### 7.7) Simple time trend (average by year)
  
```{r echo=FALSE, timetrend}

#month index
d1_21_f$month_num <- as.integer(format(d1_21_f$date, "%m"))

# Monthly means by country
m_means <- aggregate(new_cases_per_million ~ country + month_num,
                     data = d1_21_f, FUN = function(z) mean(z, na.rm = TRUE))


countries = c("United States", "United Kingdom", "China","Belgium")
mat <- with (m_means, tapply(new_cases_per_million, list(month_num, country), mean))
mat <- mat[1:12, countries, drop = FALSE]

# Set highest point to scale chart
y_max <- max(mat, na.rm = TRUE)

plot(1:12, mat[, "United States"], type = "l", lwd = 2,
     xaxt = "n", xlab = "Month in 2021",
     ylab = "mean new cases per million",
     main = "Monthly mean new cases per million",
     ylim = c(0, y_max * 1.1))

axis(1, at = 1:12, labels = month.abb)
lines(1:12, mat[, "United Kingdom"], lwd = 2)
lines(1:12, mat[, "China"], lwd = 2)
lines(1:12, mat[, "Belgium"], lwd = 2)

legend("topleft",
       legend = countries, 
       lwd = 2, bty = "n")

```
  
## 8) Planned statisical methods
We will compare 2021 distrubutions of 'new_cases_per_million' across the four countries using.... 

  
## 9) Limitations
- Measurement differences - countries have different reporting rules, testing cadence & breadth.
- Scope - Only 2021 was analysed. Other years or waves of the disease may show other patterns. 
- per million rates do not adjust for demographics of each country, which may show other patterns. 
- China has several near zero analysis - This may reflect reporting practices of this specific country


# Project Idea Two - Covid 19 Hospitalizations in France
# Link to the dataset

**Kaggle** - Coronavirusdataset France (file: `chiffres-cles.csv`)  
**Actual URL:** <https://www.kaggle.com/datasets/mclikmb4/coronavirusdataset-france?select=chiffres-cles.csv>  
**Google drive URL:** <https://drive.google.com/file/d/1rXHdGEDWFAMaitmkNSgehAt_e2FaC_PZ/view?usp=sharing>

# Introduction to the dataset

This dataset provides daily COVID-19 surveillance indicators for France at multiple geographic granularities (country, region, department, overseas collectivities). Each record includes a calendar date, a location code, and a location name, enabling comparisons across space and time. Indicators cover hospitalized patients, ICU occupancy, cumulative deaths, cumulative recoveries, and daily flows of new admissions (hospital and ICU). Source/provenance fields support auditability. The structure suits descriptive analyses and visualizations, with optional regional comparisons to highlight spatial heterogeneity. These indicators and their definitions are documented on the Kaggle dataset page (mclikmb4, 2020-2021).

# Dataset justification

**Relevance:** Directly biomedical/public-health, reflecting real-world hospital and ICU loads during COVID-19.

**Size/structure:** The file far exceeds the minimum requirements (well over 100 rows and more than 20 columns) and includes both categorical (granularity, location IDs, sources) and continuous (counts) variables.

**Accessibility/ethics:** Publicly accessible aggregated, de-identified counts suitable for academic use.

**Analytical potential:** Enables trend estimation, wave identification, geographic comparison, and lead-lag analysis between admissions (“flow”) and occupancy (“stock”).


**Ethical use.** The dataset consists of aggregated, de-identified counts without PII; no patient-level identifiers are present, aligning with course requirements for ethical, public data.


# Variables description

**Key columns:**  
`date` (daily), `granularity` (country, region, department), `location_code` (location code), `location_name` (location name).

**Indicators:**  
- `hospitalized` - current hospitalized patients  
- `icu_patients` - current ICU patients  
- `deaths` - cumulative deaths  
- `recovered` - cumulative recoveries  
- `new_hospitalizations` - new daily hospital admissions  
- `new_icu_admissions` - new daily ICU admissions  

**Additional fields:**  
`confirmed_cases` and `tested` may be present with different levels of completeness.  
**Note:** Due to several missing/invalid values (NaN/Inf), the `tested` column is largely unusable for analysis and is excluded from primary summaries and plots.

**Source metadata:**  
`source_name`, `source_url`, `source_archive`, `source_type`.

```{r results='hide'}
df <- readr::read_csv("https://drive.google.com/uc?export=download&id=1rXHdGEDWFAMaitmkNSgehAt_e2FaC_PZ", show_col_types = FALSE)

df <- df |>
  dplyr::mutate(
    granularity = dplyr::recode(granularity,
      "pays" = "country",
      "departement" = "department",
      "collectivite-outremer" = "overseas_collectivity",
      "monde" = "world",
      "region" = "region"
    )
  )


# Coerce likely numeric metrics
num_cols <- intersect(
  c("hospitalized","icu_patients","deaths","recovered",
    "new_hospitalizations","new_icu_admissions",
    "confirmed_cases","tested"),
  names(df)
)
df <- df |>
  mutate(
    date = as.Date(date),
    across(all_of(num_cols), ~suppressWarnings(as.numeric(.)))
  )

# Overview table for structure (TABLE ONLY)
gran_counts <- df |>
  count(granularity, name = "n") |>
  arrange(desc(n))

# Flattened numeric summary table (no list-columns)
summary_table <- df |>
  summarize(across(
    all_of(num_cols),
    list(
      n = ~sum(!is.na(.x)),
      mean = ~mean(.x, na.rm = TRUE),
      sd = ~sd(.x, na.rm = TRUE),
      median = ~median(.x, na.rm = TRUE),
      min = ~min(.x, na.rm = TRUE),
      max = ~max(.x, na.rm = TRUE)
    ),
    .names = "{.col}_{.fn}"
  )) |>
  pivot_longer(
    everything(),
    names_to = c("variable","stat"),
    names_sep = "_(?=[^_]+$)",
    values_to = "value"
  ) |>
  pivot_wider(names_from = "stat", values_from = "value") |>
  arrange(variable)
```

```{r}
kable(gran_counts, caption = "Row counts by geographic granularity")
kable(summary_table, digits = 3, caption = "Summary statistics for key numeric indicators")
```

# Research question(s)

1. **National waves:** How did France’s national hospitalization and ICU occupancy evolve across early pandemic waves (2020-2021)?  
2. **Flow-stock timing:** Do peaks in new hospital admissions precede peaks in current hospitalizations, and by roughly how many days?

# Data cleanup and processing plan

- **Parsing and types:** Ensure the `date` field is properly parsed as a date variable and convert indicator fields into numeric types for consistency.

- **Subsetting:** For national trends, include only rows classified as country with `location_code` = "FRA". For geographic comparisons, restrict the dataset to rows where `granularity` is region.

- **Missingness:** Quantify missing values for each column and handle them transparently by applying listwise deletion for plotted series (no imputation).

- **Duplicates:** Identify and remove duplicate entries defined by the combination of `date` and `location_code`.

- **Provenance:** Retain all source metadata fields, and include them in the appendix when relevant for transparency.

# Descriptive statistics (figures in Appendix)

France’s national indicators exhibit multi-wave patterns during 2020-2021. Hospital occupancy and ICU burden rise and fall in tandem with case surges, while cumulative deaths increase monotonically. The timing relationship between new admissions (flow) and current occupancy (stock) suggests admissions lead occupancy by several days. For visuals supporting these statements, see Appendix Figures A1-A3. Tables above summarize structure and central tendencies.

Across all rows, the median current hospitalizations was `r round(median(df$hospitalized, na.rm = TRUE))`, with an IQR of `r paste(round(quantile(df$hospitalized, probs = c(.25,.75), na.rm = TRUE)), collapse = "-")`; ICU occupancy had a much lower median, which is expected since ICU is a subset of the total hospital (median `r round(median(df$icu_patients, na.rm = TRUE))`), consistent with ICU being a subset of total hospital burden.

# Planned statistical methods

- **Lagged cross-correlation** between `new_hospitalizations` (flow) and `hospitalized` (stock) to estimate lead time from admissions to occupancy.  
- **Regional comparison** of ICU vs hospital burden by wave period (medians, IQRs).  
- **Simple time-series decomposition** on national hospitalizations to separate trend/seasonal/residual components (if applicable).


# Limitations

Several fields like `tested` and early `confirmed_cases` have bad coverage over time, and indicators are hospital-centric rather than community-representative. Counts are aggregated and de-identified, so patient-level cannot be controlled. Because the dataset mixes granularities (national, regional, departmental), comparing across levels requires careful subsetting (`granularity == "country"` for national trends). These constraints limit causal interpretation, so we have to focus more on descriptive trends and clearly labeled comparisons.

# Appendix

```{r results='hide'}
national <- df |>
  filter(granularity == "country", location_code == "FRA") |>
  arrange(date)

plot_series <- function(data, y, title_txt, ylab_txt = "Count") {
  ggplot(data, aes(x = date, y = .data[[y]])) +
    geom_line(color = "blue", linewidth = 0.8) +
    scale_x_date(date_breaks = "2 months",
                 date_labels = "%Y-%m",
                 expand = expansion(mult = c(0.01, 0.01))) +
    labs(x = "Date", y = ylab_txt, title = title_txt) +
    theme_minimal(base_size = 10) +
    theme(
      plot.title = element_text(hjust = 0.5),
      axis.title.y = element_text(margin = margin(r = 6))
    )
}

```

```{r echo=FALSE, fig.cap="France (national): Hospitalized (current) over time."}
if (all(c("hospitalized") %in% names(national))) {
  plot_series(national, "hospitalized", "France (national): Hospitalized (current) over time", ylab_txt = "Patients")
}
```

```{r echo=FALSE, fig.cap="France (national): ICU (current) over time."}
if (all(c("icu_patients") %in% names(national))) {
  plot_series(national, "icu_patients", "France (national): ICU (current) over time", ylab_txt = "Patients in ICU")
}
```

```{r echo=FALSE, fig.cap="France (national): Cumulative deaths over time."}
if (all(c("deaths") %in% names(national))) {
  plot_series(national, "deaths", "France (national): Cumulative deaths over time", ylab_txt = "Deaths (cumulative)")
}
```

\FloatBarrier
\newpage


# Project Idea Three - Heart attack


# 1.Link to the dataset

https://www.kaggle.com/datasets/iamsouravbanerjee/heart-attack-prediction-dataset

# 2. Introduction to dataset

The Heart Attack Prediction Dataset, available on Kaggle, is a comprehensive resource for studying the clinical, lifestyle, and demographic factors associated with cardiovascular risk. It consists of 8,763 de-identified patient records, including continuous variables such as age, cholesterol, blood pressure, and heart rate, as well as categorical features like sex, chest pain type, smoking habits, diabetes status, and dietary patterns. Socioeconomic and geographic attributes, including income and region, further enrich the dataset by adding broader context to heart health predictors. The primary outcome variable indicates whether a patient is at risk of a heart attack, making the dataset well-suited for statistical analysis, visualization, and classification tasks. Its diverse mix of variables supports exploration of correlations, risk factors, and group comparisons, while also providing an ethical and accessible foundation for predictive modeling in cardiovascular health research.

# 3. Dataset justification

I chose the Heart Attack Prediction Dataset because it directly addresses a critical biomedical challenge cardiovascular disease which remains one of the leading causes of mortality worldwide. The dataset integrates clinical, lifestyle, and demographic variables, making it highly relevant for exploring the multifactorial nature of heart health. With its balanced mix of categorical and continuous features, it offers strong potential for applying a variety of statistical methods, visualizations, and predictive modeling techniques. Its size and diversity of attributes make it complex enough to yield meaningful insights, yet still manageable for academic analysis. Overall, this dataset provides both real-world relevance and analytical richness, making it an excellent candidate for this project.

# 4. Variables description

Key columns include Patient ID (unique identifier for each record), Age (in years), Sex (male or female), Cholesterol (cholesterol levels in mg/dL), Blood Pressure (systolic/diastolic in mmHg), Heart Rate (beats per minute), and BMI (body mass index, kg/m²). Clinical indicators capture Diabetes status (Yes/No), Family History of heart problems (1 = Yes, 0 = No), Previous Heart Problems (1 = Yes, 0 = No), Medication Use (1 = Yes, 0 = No), and Triglyceride levels (mg/dL). Lifestyle-related attributes include Smoking (1 = Smoker, 0 = Non-smoker), Obesity (1 = Obese, 0 = Not obese), Alcohol Consumption (None, Light, Moderate, Heavy), Diet (Healthy, Average, Unhealthy), Exercise Hours Per Week, Physical Activity Days Per Week, Stress Level (1–10 scale), Sedentary Hours Per Day, and Sleep Hours Per Day. Socioeconomic and demographic fields consist of Income, Country, Continent, and Hemisphere. The target variable, Heart Attack Risk, is a binary indicator (1 = Yes, 0 = No) denoting whether the patient is at risk of a heart attack.

```{r}
# Load ggplot2 for visualizations
library(ggplot2)

# Load dataset
hd <- read.csv("data/raw/heart_attack_prediction_dataset.csv")

# View dataset structure and summary
str(hd)
summary(hd)

# Histogram of a continuous variable (Cholesterol)
ggplot(hd, aes(x = Cholesterol)) +
  geom_histogram(binwidth = 20, fill = "lightblue", color = "black") +
  labs(title = "Distribution of Cholesterol Levels",
       x = "Cholesterol (mg/dL)", y = "Count")

# Boxplot of Age grouped by Heart Attack Risk
ggplot(hd, aes(x = factor(Heart.Attack.Risk), y = Age, fill = factor(Heart.Attack.Risk))) +
  geom_boxplot() +
  labs(title = "Age Distribution by Heart Attack Risk",
       x = "Heart Attack Risk (0 = No, 1 = Yes)", y = "Age (years)") +
  scale_fill_manual(values = c("lightgreen", "red")) +
  theme_minimal()

# Bar plot of a categorical variable (Smoking)
ggplot(hd, aes(x = factor(Smoking))) +
  geom_bar(fill = "orange", color = "black") +
  labs(title = "Smoking Status of Patients",
       x = "Smoking (0 = Non-Smoker, 1 = Smoker)", y = "Count")

```

# 5. Research questions
1. Which clinical, lifestyle, and demographic factors are most strongly associated with the risk of heart attack in patients?


2. Which features contribute most to a machine learning model’s decision boundary for predicting heart attack risk?

# 6. Data cleanup and processing plan

- Check for missing values: Identify NAs using colSums(is.na(hd)); if very few, remove those rows; if moderate, impute using mean/median for continuous variables (e.g., Cholesterol, BMI) and mode for categorical variables (e.g., Diet, Alcohol Consumption).

- Remove duplicate entries: Drop exact duplicates or repeated Patient IDs to avoid over-representation using hd <- hd[!duplicated(hd), ].

- Fix inconsistent formats: Split Blood Pressure into two numeric columns (Systolic and Diastolic) and convert binary indicators (0/1) like Diabetes, Smoking, and Heart Attack Risk into categorical factors.

- Validate ranges & handle outliers: Review continuous variables (e.g., Cholesterol, Triglycerides, BMI, Sleep Hours) for biologically implausible values; correct, cap, or remove extreme outliers as appropriate.

- Standardize categorical variables: Ensure consistent levels for Sex (Male/Female), Diet (Healthy/Average/Unhealthy), and Alcohol Consumption (None/Light/Moderate/Heavy).

- Create derived variables: Add new groupings such as Age Groups (e.g., 18–30, 31–50, 51–70, 71–90) and BMI Categories (Underweight, Normal, Overweight, Obese) to facilitate group comparisons in descriptive statistics and visualization.

# 7. Descriptive statistics and data visualizations

```{r}

# Load dataset
hd <- read.csv("data/raw/heart_attack_prediction_dataset.csv")

# --- Continuous variables: Mean, Median, Range, SD ---
continuous_vars <- c("Age", "Cholesterol", "Heart.Rate", "BMI", "Triglycerides","Exercise.Hours.Per.Week", "Stress.Level","Sedentary.Hours.Per.Day", "Income", 
"Physical.Activity.Days.Per.Week", "Sleep.Hours.Per.Day")

for (var in continuous_vars) {
  cat("\nVariable:", var, "\n")
  cat("Mean:", mean(hd[[var]], na.rm=TRUE), "\n")
  cat("Median:", median(hd[[var]], na.rm=TRUE), "\n")
  cat("Range:", diff(range(hd[[var]], na.rm=TRUE)), "\n")
  cat("Standard Deviation:", sd(hd[[var]], na.rm=TRUE), "\n")
}

# --- Categorical variables: Frequency counts & proportions ---
categorical_vars <- c("Sex", "Diabetes", "Family.History", "Smoking", "Obesity","Alcohol.Consumption", "Diet", "Previous.Heart.Problems","Medication.Use", "Country", "Continent", "Hemisphere","Heart.Attack.Risk")

for (var in categorical_vars) {
  cat("\nVariable:", var, "\n")
  print(table(hd[[var]]))             
}

```

```{r}
# Function to separate categorical and numerical columns
separate_columns_by_dtype <- function(data) {
  categorical_cols <- names(data)[sapply(data, function(x) is.factor(x) || is.character(x))]
  numerical_cols   <- names(data)[sapply(data, is.numeric)]
  return(list(categorical = categorical_cols, numerical = numerical_cols))
}

# Example usage
hd <- read.csv("data/raw/heart_attack_prediction_dataset.csv", stringsAsFactors = FALSE)

cols_split <- separate_columns_by_dtype(hd)

# Verify
cat("Categorical Columns:\n")
print(cols_split$categorical)

cat("\nNumerical Columns:\n")
print(cols_split$numerical)

```



```{r}
# Load dataset
hd <- read.csv("data/raw/heart_attack_prediction_dataset.csv")

# Select only numeric variables
numeric_vars <- names(hd)[sapply(hd, is.numeric)]

# Build summary table
numeric_summary <- data.frame(
  variable = numeric_vars,
  n = sapply(hd[numeric_vars], function(x) sum(!is.na(x))),
  mean = sapply(hd[numeric_vars], function(x) round(mean(x, na.rm = TRUE), 2)),
  median = sapply(hd[numeric_vars], function(x) round(median(x, na.rm = TRUE), 2)),
  range = sapply(hd[numeric_vars], function(x) round(diff(range(x, na.rm = TRUE)), 2))
)

# Print clean summary table
print(numeric_summary)

```

# 8. Planned statistical methods

As the project progresses, I plan to use chi-square tests to assess associations between categorical factors (e.g., smoking, diabetes) and heart attack risk, and t-tests/ANOVA to compare continuous measures (e.g., cholesterol, BMI) across groups. To build predictive insight, I will apply logistic regression and may explore machine learning models such as decision trees or random forests. These methods will help identify key risk factors and evaluate their predictive power.




# ) JOINT PROJECTS - References
Project 1 - Our World in Data. "Coronvirus Pandemic (COVID-19) dataset." source location - Source: URL: https://docs.owid.io/projects/covid/en/latest/dataset.html 
Project 2 - mclikmb4, (2021, April 4), Coronavirus-dataset France, Kaggle, <https://www.kaggle.com/datasets/mclikmb4/coronavirusdataset-france?select=chiffres-cles.csv> 

Project 3 - Banerjee, S. (2021). Heart Attack Prediction Dataset. Kaggle. https://www.kaggle.com/datasets/iamsouravbanerjee/heart-attack-prediction-dataset

# ) PROJECTS - Appendix
## Project 1

## Project 2

```{r results='hide'}
national <- df |>
  filter(granularity == "country", location_code == "FRA") |>
  arrange(date)

plot_series <- function(data, y, title_txt, ylab_txt = "Count") {
  ggplot(data, aes(x = date, y = .data[[y]])) +
    geom_line(color = "blue", linewidth = 0.8) +
    scale_x_date(date_breaks = "2 months",
                 date_labels = "%Y-%m",
                 expand = expansion(mult = c(0.01, 0.01))) +
    labs(x = "Date", y = ylab_txt, title = title_txt) +
    theme_minimal(base_size = 10) +
    theme(
      plot.title = element_text(hjust = 0.5),
      axis.title.y = element_text(margin = margin(r = 6))
    )
}

```

```{r echo=FALSE, fig.cap="France (national): Hospitalized (current) over time."}
if (all(c("hospitalized") %in% names(national))) {
  plot_series(national, "hospitalized", "France (national): Hospitalized (current) over time", ylab_txt = "Patients")
}
```

```{r echo=FALSE, fig.cap="France (national): ICU (current) over time."}
if (all(c("icu_patients") %in% names(national))) {
  plot_series(national, "icu_patients", "France (national): ICU (current) over time", ylab_txt = "Patients in ICU")
}
```

```{r echo=FALSE, fig.cap="France (national): Cumulative deaths over time."}
if (all(c("deaths") %in% names(national))) {
  plot_series(national, "deaths", "France (national): Cumulative deaths over time", ylab_txt = "Deaths (cumulative)")
}
```



